{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae999376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from scipy.fft import fft, fftfreq\n",
    "#mpl.rcParams['figure.figsize'] = (10, 5)\n",
    "mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c77c1b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# 目前 TF 是否使用 GPU\n",
    "print(\"Num. GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781e5eef",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93543aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(load_file_dir:str):\n",
    "    '''讀入 csv 並存成 DataFrame'''\n",
    "    return pd.read_csv(load_file_dir, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "098f59a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safely_mkdir(dirPath:str='', silence:bool=True):\n",
    "    '''安全地生成目錄，若目錄已存在則無動作'''\n",
    "    if os.path.exists(dirPath):\n",
    "        if silence:\n",
    "            print(\"Directory is exist.\")\n",
    "    os.makedirs(dirPath, exist_ok=True) \n",
    "    return dirPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d14a60aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printlog(msg:str, dirPath:str, mode='a', sep=' ', end='\\n', flush=False):\n",
    "    '''用於輸出實驗參數設定至 txt file\n",
    "    print to std.out and \n",
    "    write to log file at the same time'''\n",
    "    print(msg, sep=sep, end=end, flush=flush)\n",
    "    with open(dirPath, mode=mode) as f:\n",
    "        print(msg, file=f, sep=sep, end=end, flush=flush)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9f6d581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyFFT(fftInput, hamming, T):\n",
    "    '''快速傅立葉變換'''\n",
    "    if hamming:\n",
    "        fftInput = fftInput * np.hamming(len(fftInput)) # hamming窗函數，減少頻率洩漏\n",
    "\n",
    "    N = len(fftInput) # Number of sample points\n",
    "    # T = 1.0 / 9.0 # sample spacing # 應使用傳入值T\n",
    "    ## x = np.linspace(0.0, N*T, N, endpoint=False)\n",
    "    y = fftInput\n",
    "    yf = fft(y)\n",
    "    xf = fftfreq(N, T)[:N//2]\n",
    "\n",
    "    yf_regular = 2.0/N * np.abs(yf[0:N//2])\n",
    "\n",
    "\n",
    "    return xf, yf_regular #*1.85 #漢明窗振幅補償"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "672e1bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_candlestick_ver1(df, show_day:np.array, SaveImgName:str):\n",
    "    '''show raw data candlestick\n",
    "    顏色採台灣股市風格：紅色表示漲, 綠色表示跌\n",
    "    資料視覺化: Ref.: https://towardsdatascience.com/basics-of-ohlc-charts-with-pythons-matplotlib-56d0e745a5be\n",
    "    Paremeter:\n",
    "    * df: DataFrame (須包含 open, close, high, low)\n",
    "    * show_day: array, [開始日:int, 結束日:int]\n",
    "    * SaveImgName:str 圖片名稱 (不用副檔名)\n",
    "    '''\n",
    "    x = np.arange(0,len(df[show_day[0]:show_day[1]]))\n",
    "    fig, ax = plt.subplots(1, figsize=(12,4))\n",
    "\n",
    "    for idx, val in df[show_day[0]:show_day[1]].iterrows():   \n",
    "        color = ('red' if val['open'] > val['close'] else 'green')\n",
    "        # high/low lines\n",
    "        plt.plot([x[idx], x[idx]], \n",
    "                [val['low'], val['high']], \n",
    "                color='black')\n",
    "        # open marker\n",
    "        plt.plot([x[idx], x[idx]-0.1], \n",
    "                [val['open'], val['open']], \n",
    "                color=color)\n",
    "        # close marker\n",
    "        plt.plot([x[idx], x[idx]+0.1], \n",
    "                [val['close'], val['close']], \n",
    "                color=color)\n",
    "\n",
    "    # ticks: date\n",
    "    #plt.xticks(x[::3], df_apple.date.dt.date[::3])\n",
    "    ax.set_xticks(x, minor=True) # apply to every ticks\n",
    "\n",
    "    # labels\n",
    "    plt.ylabel('USD')\n",
    "\n",
    "    # grid\n",
    "    ax.xaxis.grid(color='black', linestyle='dashed', which='both', alpha=0.1)\n",
    "\n",
    "    # remove spines\n",
    "    #ax.spines['right'].set_visible(False)\n",
    "    #ax.spines['top'].set_visible(False)\n",
    "\n",
    "    # title\n",
    "    plt.title('IBM Stock Price', loc='left', fontsize=20)\n",
    "    safely_mkdir('./img')\n",
    "    plt.savefig(f'./img/{SaveImgName}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c0ca683",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import mplfinance as mpf\n",
    "def visualize_candlestick_ver2(df=None, type='candle', mav=[5], show_day=None):\n",
    "    '''Use mpf package to show candlestick\n",
    "    K 線圖 (Candlestick Chart)\n",
    "    均線: moving averages with the mav keyword, [5日均線]\n",
    "    ---\n",
    "    Paremter:\n",
    "    * df: DataFrame (須包含 open, close, high, low)\n",
    "    * type: 顯示模式\n",
    "    * mav: 均線\n",
    "    * show_day: array, [開始日:int, 結束日:int]\n",
    "    '''\n",
    "    #df = df.copy()\n",
    "    #df.index = pd.to_datetime(df.index, unit=\"D\")\n",
    "    #mpf.plot(df[show_day[0]:show_day[1]], type=type, mav=mav, datetime_format='%b %d')\n",
    "    #print(\"因資料集無法獲得確切時間，因此 x 軸並非真實日期\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c58580f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_add_moving_average_columns(raw_df):\n",
    "    # Add MOVING AVERAGE Columns (計算日均值)\n",
    "    raw_df['open-MA 5 days'] = raw_df['open'].rolling(5).mean()\n",
    "    raw_df['open-MA 10 days'] = raw_df['open'].rolling(10).mean()\n",
    "    raw_df['open-MA 20 days'] = raw_df['open'].rolling(20).mean()\n",
    "    raw_df['open-MA 60 days'] = raw_df['open'].rolling(60).mean()\n",
    "    raw_df['open-MA 120 days'] = raw_df['open'].rolling(120).mean()\n",
    "\n",
    "    raw_df['close-MA 5 days'] = raw_df['close'].rolling(5).mean()\n",
    "    raw_df['close-MA 10 days'] = raw_df['close'].rolling(10).mean()\n",
    "    raw_df['close-MA 20 days'] = raw_df['close'].rolling(20).mean()\n",
    "    raw_df['close-MA 60 days'] = raw_df['close'].rolling(60).mean()\n",
    "    raw_df['close-MA 120 days'] = raw_df['close'].rolling(120).mean()\n",
    "    \n",
    "    # 移除最前面計算平均時, 因天數不足導致 MA 欄位為 NaN 的資料 \n",
    "    # Removing all the NULL values using dropna() method\n",
    "    raw_df.dropna(inplace=True)\n",
    "\n",
    "    return raw_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4204fb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateDataset(Raw_Data_df:pd.DataFrame, \n",
    "                    offset:int, num_features:int, columns_to_be_label:list, predict_num_days:int,\n",
    "                    test_mode:bool, NewAppendData:pd.core.series.Series,\n",
    "                    print_explain:bool):\n",
    "    '''功能 1: 產生新的盛裝 Numpy 資料集 (用 dict 成裝, 訓練集或測試集都可製作)\n",
    "    功能 2: 當輸入『現有的 numpy 資料集』及『(預測結束後) 當天的原始資料』，就能製作一筆新的 numpy 資料並 append 至現有的 numpy 資料集\n",
    "    ---\n",
    "    Paremeters:\n",
    "    * 功能 1:\n",
    "      * Raw_Data_df (DataFrame): 原始 pandas df 資料表\n",
    "      * offset (int): 用於產生 input x, offset 即 lstm 的 steps,表示欲製作的 numpy 資料集，要往前看幾天份的資料 （往前看 offset 天）\n",
    "      * num_features (int): 用於產生 input x, 特徵數量\n",
    "      * columns_to_be_label (list of str): 用於產生 label y, 此為原始 pandas df 資料表中要選作為 label 的「欄位名稱」\n",
    "      * predict_num_days (int): 預測未來幾天\n",
    "    * 功能 2 (只會於 model inference 之後使用):\n",
    "      * test_mode (bool): 表示現在是否為 testing 模式 (即使用功能 2)\n",
    "      * NewAppendData (one sample of pd.DataFrame): 今天收盤後的「今日資料」, 用於產生新的 numpy 資料集\n",
    "    * print_explain (bool): 是否要印出解釋數據？\n",
    "    ---\n",
    "    Returns: \n",
    "    * NumpyDataset_dict (dict of numpy arrays): 新的 Numpu Dataset\n",
    "    * New_Data_df : 新的 raw data pandas 資料表 (若使用功能 2, 此表會新增一筆資料)\n",
    "    '''\n",
    "\n",
    "    if not test_mode:\n",
    "      # 初始化: Numpy 資料集\n",
    "      NumpyDataset_dict = {'x':np.empty(shape=(0,offset,num_features)), 'y':np.empty(shape=(0,2*predict_num_days,1))}\n",
    "\n",
    "      # 逐筆製作 numpy 資料並 append 至 Numpy 資料集\n",
    "      for date in range(1, len(Raw_Data_df)-offset-predict_num_days+1):\n",
    "        # 製作一筆 x data: 每 offset 天的 features 製作成一筆 x data\n",
    "        one_sample_x_np = Raw_Data_df[date:date+offset].to_numpy()\n",
    "        NumpyDataset_dict['x'] = np.append(NumpyDataset_dict['x'], one_sample_x_np[np.newaxis,:], axis=0)\n",
    "        # 製作一筆 y data: 將未來的？天的開盤價及收盤價製作成一筆 y data\n",
    "        # 初始化: y data \n",
    "        one_sample_y_np = np.empty(shape=(0, 1))\n",
    "        for future_date in range(predict_num_days):\n",
    "          # 依欲預測的未來天數, 逐日取出 label data\n",
    "          one_sample_y_np = np.append(one_sample_y_np, Raw_Data_df[columns_to_be_label].iloc[date+offset+future_date].to_numpy()[:,np.newaxis], axis=0)\n",
    "        NumpyDataset_dict['y'] = np.append(NumpyDataset_dict['y'], one_sample_y_np[np.newaxis,:], axis=0)\n",
    "    else: # is Testing mode\n",
    "      # 初始化: Numpy 資料集\n",
    "      NumpyDataset_dict = {'x':np.empty(shape=(0,offset,num_features))}\n",
    "\n",
    "      # --- Preprocess 1: 轉 pandas ---\n",
    "      NewAppendData = pd.DataFrame(NewAppendData).T\n",
    "\n",
    "      # --- Preprocess 2: 欄位重新命名 ---\n",
    "      NewAppendData.rename(columns={0:'open', 1:'high', 2:'low', 3:'close'}, inplace=True)\n",
    "\n",
    "      # --- Preprocess 3: 將今日新增資料合併至 Training Data ---\n",
    "      New_Data_df = pd.concat([Raw_Data_df.copy(), NewAppendData], ignore_index=True)\n",
    "\n",
    "      # --- Preprocess 4: ---\n",
    "      # * Add MOVING AVERAGE Columns (計算日均值)\n",
    "      New_Data_df.loc[New_Data_df.shape[0]-1:,'open-MA 5 days'] = New_Data_df.loc[New_Data_df.shape[0]-5:New_Data_df.shape[0]-1,'open'].mean()\n",
    "      New_Data_df.loc[New_Data_df.shape[0]-1:,'open-MA 10 days'] = New_Data_df.loc[New_Data_df.shape[0]-10:New_Data_df.shape[0]-1,'open'].mean()\n",
    "      New_Data_df.loc[New_Data_df.shape[0]-1:,'open-MA 20 days'] = New_Data_df.loc[New_Data_df.shape[0]-20:New_Data_df.shape[0]-1,'open'].mean()\n",
    "      New_Data_df.loc[New_Data_df.shape[0]-1:,'open-MA 60 days'] = New_Data_df.loc[New_Data_df.shape[0]-60:New_Data_df.shape[0]-1,'open'].mean()\n",
    "      New_Data_df.loc[New_Data_df.shape[0]-1:,'open-MA 120 days'] = New_Data_df.loc[New_Data_df.shape[0]-120:New_Data_df.shape[0]-1,'open'].mean()\n",
    "\n",
    "      New_Data_df.loc[New_Data_df.shape[0]-1:,'close-MA 5 days'] = New_Data_df.loc[New_Data_df.shape[0]-5:New_Data_df.shape[0]-1,'close'].mean()\n",
    "      New_Data_df.loc[New_Data_df.shape[0]-1:,'close-MA 10 days'] = New_Data_df.loc[New_Data_df.shape[0]-10:New_Data_df.shape[0]-1,'close'].mean()\n",
    "      New_Data_df.loc[New_Data_df.shape[0]-1:,'close-MA 20 days'] = New_Data_df.loc[New_Data_df.shape[0]-20:New_Data_df.shape[0]-1,'close'].mean()\n",
    "      New_Data_df.loc[New_Data_df.shape[0]-1:,'close-MA 60 days'] = New_Data_df.loc[New_Data_df.shape[0]-60:New_Data_df.shape[0]-1,'close'].mean()\n",
    "      New_Data_df.loc[New_Data_df.shape[0]-1:,'close-MA 120 days'] = New_Data_df.loc[New_Data_df.shape[0]-120:New_Data_df.shape[0]-1,'close'].mean()\n",
    "\n",
    "      '''\n",
    "      check_for_nan = New_Data_df.isnull().values.any()\n",
    "      if check_for_nan:\n",
    "        print(New_Data_df)\n",
    "        raise Exception(ValueError)\n",
    "      '''\n",
    "      '''\n",
    "      print(f\"old shape: {New_Data_df.shape}\")\n",
    "      # 移除最前面計算平均時, 因天數不足導致 MA 欄位為 NaN 的資料 \n",
    "      # Removing all the NULL values using dropna() method\n",
    "      New_Data_df.dropna(inplace=True)\n",
    "      print(f\"new shape: {New_Data_df.shape}\")\n",
    "      '''\n",
    "\n",
    "      # --- 製作 Numpy Testing Dataset ---\n",
    "      # 只取最後 offset 筆資料來製作成資料集 (若要製作預測未來 predict_num_days 天，則要往前取 offset + predict_num_days 天)\n",
    "      \n",
    "      # 製作一筆 x data: 取  offset + predict_num_days -1 天的 features 製作成一筆 x data\n",
    "      one_sample_x_np = New_Data_df[-1*offset:].to_numpy()\n",
    "      NumpyDataset_dict['x'] = one_sample_x_np[np.newaxis,:]\n",
    "      \n",
    "      '''\n",
    "      try:\n",
    "        NumpyDataset_dict['x'] = np.append(NumpyDataset_dict['x'], one_sample_x_np[np.newaxis,:], axis=0)\n",
    "      except Exception:\n",
    "        raise NameError('HiThere')\n",
    "      #NumpyDataset_dict['x'] = one_sample_x_np[np.newaxis,:]\n",
    "      '''\n",
    "      \n",
    "    \n",
    "    if print_explain:\n",
    "      print(f\"資料集 x shpae = {NumpyDataset_dict['x'].shape}:\\n\" \\\n",
    "            f\"{NumpyDataset_dict['x'].shape[0]} samples(資料筆數),\\n\" \\\n",
    "            f\"{NumpyDataset_dict['x'].shape[1]} steps({offset}天),\\n\" \\\n",
    "            f\"{NumpyDataset_dict['x'].shape[2]} features(特徵數量)\\n\")\n",
    "      if not test_mode:\n",
    "        print(f\"資料集 y shpae = {NumpyDataset_dict['y'].shape}:\\n\" \\\n",
    "              f\"{NumpyDataset_dict['y'].shape[0]} samples(資料筆數),\\n\" \\\n",
    "              f\"{NumpyDataset_dict['y'].shape[1]} labels(特徵數量) 0:明天開盤價, 1:明天收盤價, 依此類推\\n\" \\\n",
    "              f\"{NumpyDataset_dict['y'].shape[2]} (開盤價/收盤價)\")\n",
    "\n",
    "    if not test_mode:\n",
    "      return NumpyDataset_dict\n",
    "    else:\n",
    "      return NumpyDataset_dict, New_Data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f26e3f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_Model(input_dims, output_dims, num_output_layer, name, model_type, ):\n",
    "    '''Functional API format: \n",
    "    keras functional api is a highly customizable design.\n",
    "    This is Kuihao's style model builder.'''\n",
    "    # Define input layer with shape \n",
    "    input_layer = tf.keras.Input(shape=input_dims) \n",
    "\n",
    "    # Construct NN connections\n",
    "    x = input_layer # apply to input layer\n",
    "    \n",
    "    if model_type == 'LSTM':\n",
    "        x = tf.keras.layers.LSTM(units=20, activation='relu', return_sequences=True)(x)\n",
    "        x = tf.keras.layers.LSTM(units=10, activation='relu', return_sequences=False)(x)\n",
    "\n",
    "    if model_type == 'GRU':\n",
    "        x = tf.keras.layers.GRU(units=20, activation='relu', return_sequences=True)(x)\n",
    "        x = tf.keras.layers.GRU(units=10, activation='relu', return_sequences=False)(x)\n",
    "    \n",
    "    # Concatenate other NN\n",
    "    #x = tf.keras.layers.Concatenate()([x])\n",
    "    \n",
    "    # apply to outout layer with shape\n",
    "    if num_output_layer == 1:\n",
    "        output_layer = tf.keras.layers.Dense(output_dims, activation='relu')(x)\n",
    "        return tf.keras.Model(inputs=[input_layer], outputs=[output_layer], name=name)\n",
    "    else:\n",
    "        output_layer_1 = tf.keras.layers.Dense(output_dims, activation='relu')(x)\n",
    "        output_layer_2 = tf.keras.layers.Dense(output_dims, activation='relu')(x)\n",
    "        return tf.keras.Model(inputs=[input_layer], outputs=[output_layer_1, output_layer_2], name=name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d338604",
   "metadata": {},
   "source": [
    "## Stock Strategy\n",
    "買賣決策\n",
    "* 提供參考: [KD指標-KD黃金交叉建議做多-死亡交叉建議做空](https://ithelp.ithome.com.tw/articles/10206894)\n",
    "Inference 注意事項\n",
    "* 依規定必須「輸出明日模型預測」和「讀取一天資料」**輪流進行**\n",
    "* output.csv 不可以預先讀入，必須完全由 code 執行時產生並輸出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fa19fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df30ffaa",
   "metadata": {},
   "source": [
    "## Trader class\n",
    "依範例程式，可以寫一個名為 Trader 的 class 方便助教檢驗程式<br>\n",
    "此類別會實作下列方法 (method):\n",
    "* train(load_training_df:pd.DataFrame, hyperparameter:list, usePreTrain:bool): 匯入訓練資料並執行 model 訓練\n",
    "    * 新增參數: \n",
    "        * hyperparameter: lr, optimizer, random seed...etc\n",
    "        * usePreTrain: 是否使用預訓練模型\n",
    "    * load data preprocess\n",
    "    * build model\n",
    "    * load model\n",
    "    * model training\n",
    "    * model save\n",
    "* predict_action(row): 依讀入的單行資料進行 model inference\n",
    "    * 讀入當天資料之後，進行簡單的前處理，才進行模型預測、決策邏輯，再輸出明天的操盤行為 (action)\n",
    "    * 需要將本日讀入的資料放進訓練集之中\n",
    "* re_training(doReTrain:bool): 使用本日資料重新 Training\n",
    "    * 新增參數:\n",
    "        * doReTrain: 是否執行重新訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b87d858",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trader():\n",
    "    def __init__(self, StartDatetime_str):\n",
    "        self.StartDatetime_str = StartDatetime_str # 此程式開始執行的時間，用於指定模型儲存路徑，不會用於訓練/預測\n",
    "        self.raw_training_df = None # 原始匯入的 Training Data\n",
    "        self.Train_set_1 = None # dict of numpy dataset (每 120 天預測未來 1 天) \n",
    "        self.Train_set_2 = None # dict of numpy dataset (每 120 天預測未來 2 天)\n",
    "        self.model_1 = None # (取近期 120 天預測未來 1 天) \n",
    "        self.model_2 = None # (取近期 120 天預測未來 2 天) \n",
    "        self.Test_set = None # dict of numpy dataset (最近的 120 天，只有 x 無 y) \n",
    "\n",
    "    def train(self, load_training_df:pd.DataFrame=None, \n",
    "              hyperparameter:list=None, usePreTrain:bool=True,\n",
    "              choosen_model:list=[True,True]):\n",
    "        '''\n",
    "        Parameter:\n",
    "        * load_training_df: 匯入的 training_data.csv 轉成的最原始 DataFrame\n",
    "        * hyperparameter: 時間因素，並無實作 hyperparameter 參數\n",
    "        * usePreTrain: bool, 是否要使用預訓練模型，預設開啟 (不會從頭開始 Training)\n",
    "        * choosen_model: 要使用 model 1 (只預測明天) 還是 model 2 (預測明天+後天)，預設兩個我都要\n",
    "        '''\n",
    "        # --- Import raw trauining data ---\n",
    "        self.raw_training_df = load_training_df\n",
    "\n",
    "        # --- Preprocess 1: 欄位重新命名 ---\n",
    "        self.raw_training_df.rename(columns={0:'open', 1:'high', 2:'low', 3:'close'}, inplace=True)\n",
    "\n",
    "        # --- Preprocess 2: ---\n",
    "        # * Add MOVING AVERAGE Columns (計算日均值)\n",
    "        # * 移除最前面計算平均時, 因天數不足導致 MA 欄位為 NaN 的資料 \n",
    "        self.raw_training_df = preprocess_add_moving_average_columns(self.raw_training_df)\n",
    "\n",
    "        # --- Make Dataset ---\n",
    "        if choosen_model[0]:\n",
    "            # [Train_set_1] 往前回推，每 120 天,預測未來 1 天\n",
    "            self.Train_set_1 = GenerateDataset(self.raw_training_df,\n",
    "                                            offset=120, num_features=len(self.raw_training_df.columns), \n",
    "                                            columns_to_be_label=['open','close'], predict_num_days=1,\n",
    "                                            test_mode=False, NewAppendData=None,\n",
    "                                            print_explain=False)\n",
    "        if choosen_model[1]:\n",
    "            # [Train_set_2] 往前回推，每 120 天,預測未來 2 天\n",
    "            self.Train_set_2 = GenerateDataset(self.raw_training_df,\n",
    "                                            offset=120, num_features=len(self.raw_training_df.columns), \n",
    "                                            columns_to_be_label=['open','close'], predict_num_days=2,\n",
    "                                            test_mode=False, NewAppendData=None,\n",
    "                                            print_explain=False)\n",
    "\n",
    "        if not usePreTrain:\n",
    "            # --- Build Model --- \n",
    "            if choosen_model[0]:\n",
    "                self.model_1 = Build_Model(input_dims=(120,14), output_dims=2, name='Stock_Price_tomorrow', \n",
    "                                           num_output_layer=1, model_type='GRU')\n",
    "            if choosen_model[1]:\n",
    "                self.model_2 = Build_Model(input_dims=(120,14), output_dims=2, name='Stock_Price_2_days',\n",
    "                                           num_output_layer=2, model_type='GRU')\n",
    "            \n",
    "            myAdam =  tf.keras.optimizers.Adam(learning_rate=1e-4,)\n",
    "            RMSE_loss = tf.keras.metrics.RootMeanSquaredError(name='root_mean_squared_error', dtype=None)\n",
    "            if choosen_model[0]: self.model_1.compile(optimizer=myAdam, loss='mse',  metrics=[RMSE_loss])\n",
    "            if choosen_model[1]: self.model_2.compile(optimizer=myAdam, loss='mse',  metrics=[RMSE_loss])\n",
    "            \n",
    "            # --- Train Model --- \n",
    "            if choosen_model[0]: \n",
    "                self.model_1.fit(self.Train_set_1['x'], \n",
    "                                self.Train_set_1['y'],\n",
    "                                epochs=120,\n",
    "                                batch_size=32,)\n",
    "                self.model_1.save(f'./model/model1_{self.StartDatetime_str}.h5')\n",
    "            if choosen_model[1]:\n",
    "                self.model_2.fit(self.Train_set_2['x'], \n",
    "                                [self.Train_set_2['y'][:,:2], self.Train_set_2['y'][:,2:]],\n",
    "                                epochs=150,\n",
    "                                batch_size=32,)\n",
    "                self.model_2.save(f'./model/model2_{self.StartDatetime_str}.h5')    \n",
    "        else:\n",
    "            # --- 匯入最佳模型 (pre-train model) ---\n",
    "            if choosen_model[0]:\n",
    "                # Load Train Model 1\n",
    "                self.model_1 = tf.keras.models.load_model(f'./model/model1_2022_04_21__21_08_52.h5')\n",
    "            if choosen_model[1]:\n",
    "                # Load Train Model 2\n",
    "                self.model_2 = tf.keras.models.load_model(f'./model/model2_2022_04_21__21_08_52.h5')\n",
    "\n",
    "    def predict_action(self, row):\n",
    "        \"\"\"\n",
    "        row: 今日的 IBM stock 資料 (只有一行)\n",
    "        1. 對今日資料進行簡單的前處理以符合 model input dims\n",
    "        2. 將今日資料合併至舊的 training dataset 以方便取最後倒數 120 天的資料\n",
    "           製作出 Testing dataset\n",
    "        \"\"\"\n",
    "        # --- Make dataset: 製作 Testing set ---\n",
    "        # [Test_set] 往前回推 120 天\n",
    "        self.Test_set, self.raw_training_df = GenerateDataset(self.raw_training_df,\n",
    "                                                offset=120, num_features=len(self.raw_training_df.columns), \n",
    "                                                columns_to_be_label=['open','close'], predict_num_days=1,\n",
    "                                                test_mode=True, NewAppendData=row,\n",
    "                                                print_explain=False)\n",
    "\n",
    "        #predict = self.model_1.predict(self.Test_set['x'][-1:]) # 輸出明天開盤價、收盤價\n",
    "        predict = self.model_2.predict(self.Test_set['x'][-1:]) # 輸出明天及後天開盤價、收盤價\n",
    "        #print(predict)\n",
    "        #print(self.raw_training_df.shape)\n",
    "\n",
    "        # ----------------- #\n",
    "        # 嵌入買賣決策程式碼 #\n",
    "        # ---------------- #\n",
    "\n",
    "        action = str(0) # 可輸出: 0, 1, -1\n",
    "        return action\n",
    "\n",
    "    def re_training(self, doReTrain:bool, choosen_model:list):\n",
    "        \"\"\"將最新取得的測試資料進行 model 二次訓練\n",
    "        Paremeter:\n",
    "        * doReTrain:bool  是否要 re-train\n",
    "        * choosen_model:list of bool  要 re-train 的 model 填 True\n",
    "        \"\"\"\n",
    "        if doReTrain:\n",
    "            if choosen_model[0]:\n",
    "                # 重新進行資料前處理 re-generate training dataset\n",
    "                self.Train_set_1 = GenerateDataset(self.raw_training_df,\n",
    "                                                   offset=120, num_features=len(self.raw_training_df.columns), \n",
    "                                                   columns_to_be_label=['open','close'], predict_num_days=1,\n",
    "                                                   test_mode=False, NewAppendData=None,\n",
    "                                                   print_explain=False)\n",
    "\n",
    "                self.model_1.fit(self.Train_set_1['x'], \n",
    "                                 self.Train_set_1['y'],\n",
    "                                 epochs=120,\n",
    "                                 batch_size=32,)\n",
    "                self.model_1.save(f'./model/model1_{self.StartDatetime_str}.h5')\n",
    "            if choosen_model[1]:\n",
    "                # 重新進行資料前處理 re-generate training dataset\n",
    "                self.Train_set_2 = GenerateDataset(self.raw_training_df,\n",
    "                                    offset=30, num_features=len(self.raw_training_df.columns), \n",
    "                                    columns_to_be_label=['open','close'], predict_num_days=2,\n",
    "                                    test_mode=False, NewAppendData=None,\n",
    "                                    print_explain=False)\n",
    "\n",
    "                self.model_2.fit(self.Train_set_2['x'], \n",
    "                                 [self.Train_set_2['y'][:,:2], self.Train_set_2['y'][:,2:]],\n",
    "                                 epochs=30,\n",
    "                                 batch_size=32,)\n",
    "                self.model_2.save(f'./model/model2_{self.StartDatetime_str}.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e2d09f",
   "metadata": {},
   "source": [
    "## 實驗測試 main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69fa3da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2022-04-22 14:12:07.409322\n",
      "Random seed: [3317294818]\n",
      "Train on 1249 samples\n",
      "Epoch 1/120\n",
      "1249/1249 [==============================] - 4s 3ms/sample - loss: 9.5650 - root_mean_squared_error: 3.0927\n",
      "Epoch 2/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 9.2551 - root_mean_squared_error: 3.0422\n",
      "Epoch 3/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 9.2463 - root_mean_squared_error: 3.0408\n",
      "Epoch 4/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 9.2832 - root_mean_squared_error: 3.0468\n",
      "Epoch 5/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 9.2848 - root_mean_squared_error: 3.0471\n",
      "Epoch 6/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 9.1125 - root_mean_squared_error: 3.0187\n",
      "Epoch 7/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 9.0259 - root_mean_squared_error: 3.0043\n",
      "Epoch 8/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 8.9438 - root_mean_squared_error: 2.9906\n",
      "Epoch 9/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 9.1493 - root_mean_squared_error: 3.0248\n",
      "Epoch 10/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 8.8081 - root_mean_squared_error: 2.9678\n",
      "Epoch 11/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 8.8387 - root_mean_squared_error: 2.9730\n",
      "Epoch 12/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 8.7837 - root_mean_squared_error: 2.9637\n",
      "Epoch 13/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 8.8398 - root_mean_squared_error: 2.9732\n",
      "Epoch 14/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 8.6562 - root_mean_squared_error: 2.9421\n",
      "Epoch 15/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 8.6952 - root_mean_squared_error: 2.9488\n",
      "Epoch 16/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 9.0228 - root_mean_squared_error: 3.0038\n",
      "Epoch 17/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 8.6268 - root_mean_squared_error: 2.9371\n",
      "Epoch 18/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 8.6095 - root_mean_squared_error: 2.9342\n",
      "Epoch 19/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 8.5984 - root_mean_squared_error: 2.9323\n",
      "Epoch 20/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 8.4345 - root_mean_squared_error: 2.9042\n",
      "Epoch 21/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 8.4861 - root_mean_squared_error: 2.9131\n",
      "Epoch 22/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 8.4066 - root_mean_squared_error: 2.8994\n",
      "Epoch 23/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 8.5531 - root_mean_squared_error: 2.9246\n",
      "Epoch 24/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 8.5152 - root_mean_squared_error: 2.9181\n",
      "Epoch 25/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 8.5620 - root_mean_squared_error: 2.9261\n",
      "Epoch 26/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 8.4484 - root_mean_squared_error: 2.9066\n",
      "Epoch 27/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 8.2399 - root_mean_squared_error: 2.8705\n",
      "Epoch 28/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 8.2768 - root_mean_squared_error: 2.8769\n",
      "Epoch 29/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 8.4047 - root_mean_squared_error: 2.8991\n",
      "Epoch 30/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 8.2997 - root_mean_squared_error: 2.8809\n",
      "Epoch 31/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 8.2501 - root_mean_squared_error: 2.8723\n",
      "Epoch 32/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 8.2000 - root_mean_squared_error: 2.8636\n",
      "Epoch 33/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 8.2545 - root_mean_squared_error: 2.8731\n",
      "Epoch 34/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 8.5698 - root_mean_squared_error: 2.9274\n",
      "Epoch 35/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 8.6082 - root_mean_squared_error: 2.9340\n",
      "Epoch 36/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 8.4028 - root_mean_squared_error: 2.8988\n",
      "Epoch 37/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 7.9396 - root_mean_squared_error: 2.8177\n",
      "Epoch 38/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 8.1630 - root_mean_squared_error: 2.8571\n",
      "Epoch 39/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 8.0154 - root_mean_squared_error: 2.8312\n",
      "Epoch 40/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 7.8726 - root_mean_squared_error: 2.8058\n",
      "Epoch 41/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 8.0057 - root_mean_squared_error: 2.8294\n",
      "Epoch 42/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 7.8955 - root_mean_squared_error: 2.8099\n",
      "Epoch 43/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 7.9309 - root_mean_squared_error: 2.8162\n",
      "Epoch 44/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 7.9309 - root_mean_squared_error: 2.8162\n",
      "Epoch 45/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 8.0905 - root_mean_squared_error: 2.8444\n",
      "Epoch 46/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 7.8685 - root_mean_squared_error: 2.8051\n",
      "Epoch 47/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 8.4592 - root_mean_squared_error: 2.9085\n",
      "Epoch 48/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 7.7666 - root_mean_squared_error: 2.7869\n",
      "Epoch 49/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 7.6730 - root_mean_squared_error: 2.7700\n",
      "Epoch 50/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 7.6997 - root_mean_squared_error: 2.7748\n",
      "Epoch 51/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 7.9290 - root_mean_squared_error: 2.8158\n",
      "Epoch 52/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 7.8979 - root_mean_squared_error: 2.8103\n",
      "Epoch 53/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 7.6993 - root_mean_squared_error: 2.7748\n",
      "Epoch 54/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 7.6635 - root_mean_squared_error: 2.7683\n",
      "Epoch 55/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 8.2898 - root_mean_squared_error: 2.8792\n",
      "Epoch 56/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 7.7942 - root_mean_squared_error: 2.7918\n",
      "Epoch 57/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 7.5944 - root_mean_squared_error: 2.7558\n",
      "Epoch 58/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 8.0457 - root_mean_squared_error: 2.8365\n",
      "Epoch 59/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 7.8692 - root_mean_squared_error: 2.8052\n",
      "Epoch 60/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 7.9220 - root_mean_squared_error: 2.8146\n",
      "Epoch 61/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 7.5621 - root_mean_squared_error: 2.7499\n",
      "Epoch 62/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 7.4597 - root_mean_squared_error: 2.7312\n",
      "Epoch 63/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 7.5831 - root_mean_squared_error: 2.7537\n",
      "Epoch 64/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 7.6183 - root_mean_squared_error: 2.7601\n",
      "Epoch 65/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 7.5438 - root_mean_squared_error: 2.7466\n",
      "Epoch 66/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 7.5783 - root_mean_squared_error: 2.7529\n",
      "Epoch 67/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 8.0020 - root_mean_squared_error: 2.8288\n",
      "Epoch 68/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 7.4508 - root_mean_squared_error: 2.7296\n",
      "Epoch 69/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 7.3751 - root_mean_squared_error: 2.7157\n",
      "Epoch 70/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 7.3279 - root_mean_squared_error: 2.7070\n",
      "Epoch 71/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 7.3630 - root_mean_squared_error: 2.7135\n",
      "Epoch 72/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 7.2380 - root_mean_squared_error: 2.6903\n",
      "Epoch 73/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 7.3671 - root_mean_squared_error: 2.7142\n",
      "Epoch 74/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 7.6188 - root_mean_squared_error: 2.7602\n",
      "Epoch 75/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 7.6801 - root_mean_squared_error: 2.7713\n",
      "Epoch 76/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 7.4828 - root_mean_squared_error: 2.7355\n",
      "Epoch 77/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 8.4203 - root_mean_squared_error: 2.9018\n",
      "Epoch 78/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 7.3993 - root_mean_squared_error: 2.7202\n",
      "Epoch 79/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 7.4595 - root_mean_squared_error: 2.7312\n",
      "Epoch 80/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 7.2833 - root_mean_squared_error: 2.6988\n",
      "Epoch 81/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 7.0726 - root_mean_squared_error: 2.6594\n",
      "Epoch 82/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 7.2443 - root_mean_squared_error: 2.6915\n",
      "Epoch 83/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 7.2510 - root_mean_squared_error: 2.6928\n",
      "Epoch 84/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 7.0018 - root_mean_squared_error: 2.6461\n",
      "Epoch 85/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 7.1608 - root_mean_squared_error: 2.6760\n",
      "Epoch 86/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 6.9857 - root_mean_squared_error: 2.6431\n",
      "Epoch 87/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 7.2720 - root_mean_squared_error: 2.6967\n",
      "Epoch 88/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 7.2194 - root_mean_squared_error: 2.6869\n",
      "Epoch 89/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 7.0714 - root_mean_squared_error: 2.6592\n",
      "Epoch 90/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 6.9225 - root_mean_squared_error: 2.6311\n",
      "Epoch 91/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 7.0955 - root_mean_squared_error: 2.6637\n",
      "Epoch 92/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 7.1355 - root_mean_squared_error: 2.6712\n",
      "Epoch 93/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 7.1848 - root_mean_squared_error: 2.6804\n",
      "Epoch 94/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 7.5612 - root_mean_squared_error: 2.7498\n",
      "Epoch 95/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 6.9498 - root_mean_squared_error: 2.6363\n",
      "Epoch 96/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 6.8391 - root_mean_squared_error: 2.6152\n",
      "Epoch 97/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 7.0985 - root_mean_squared_error: 2.6643\n",
      "Epoch 98/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 6.8229 - root_mean_squared_error: 2.6121\n",
      "Epoch 99/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 7.3486 - root_mean_squared_error: 2.7108\n",
      "Epoch 100/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 6.8132 - root_mean_squared_error: 2.6102\n",
      "Epoch 101/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 8.2694 - root_mean_squared_error: 2.8757\n",
      "Epoch 102/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 7.4528 - root_mean_squared_error: 2.7300\n",
      "Epoch 103/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 6.9469 - root_mean_squared_error: 2.6357\n",
      "Epoch 104/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 6.8669 - root_mean_squared_error: 2.6205\n",
      "Epoch 105/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 6.8386 - root_mean_squared_error: 2.6151\n",
      "Epoch 106/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 6.9150 - root_mean_squared_error: 2.6296\n",
      "Epoch 107/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 6.8600 - root_mean_squared_error: 2.6192\n",
      "Epoch 108/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 6.7604 - root_mean_squared_error: 2.6001\n",
      "Epoch 109/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 6.8603 - root_mean_squared_error: 2.6192\n",
      "Epoch 110/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 7.4687 - root_mean_squared_error: 2.7329\n",
      "Epoch 111/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 6.6873 - root_mean_squared_error: 2.5860\n",
      "Epoch 112/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 7.0361 - root_mean_squared_error: 2.6526\n",
      "Epoch 113/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 6.6405 - root_mean_squared_error: 2.5769\n",
      "Epoch 114/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 6.5518 - root_mean_squared_error: 2.5597\n",
      "Epoch 115/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 6.6859 - root_mean_squared_error: 2.5857\n",
      "Epoch 116/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 6.6179 - root_mean_squared_error: 2.5725\n",
      "Epoch 117/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 6.5738 - root_mean_squared_error: 2.5640\n",
      "Epoch 118/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 6.5997 - root_mean_squared_error: 2.5690\n",
      "Epoch 119/120\n",
      "1249/1249 [==============================] - 3s 2ms/sample - loss: 6.5207 - root_mean_squared_error: 2.5536\n",
      "Epoch 120/120\n",
      "1249/1249 [==============================] - 3s 3ms/sample - loss: 6.7649 - root_mean_squared_error: 2.6009\n",
      "Train on 1248 samples\n",
      "Epoch 1/150\n",
      "1248/1248 [==============================] - 3s 3ms/sample - loss: 25.5120 - dense_4_loss: 13.5790 - dense_5_loss: 11.9330 - dense_4_root_mean_squared_error: 3.6850 - dense_5_root_mean_squared_error: 3.4544\n",
      "Epoch 2/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 25.6953 - dense_4_loss: 13.6204 - dense_5_loss: 12.0749 - dense_4_root_mean_squared_error: 3.6906 - dense_5_root_mean_squared_error: 3.4749\n",
      "Epoch 3/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 25.3705 - dense_4_loss: 13.4374 - dense_5_loss: 11.9331 - dense_4_root_mean_squared_error: 3.6657 - dense_5_root_mean_squared_error: 3.4544\n",
      "Epoch 4/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 25.3007 - dense_4_loss: 13.4037 - dense_5_loss: 11.8970 - dense_4_root_mean_squared_error: 3.6611 - dense_5_root_mean_squared_error: 3.4492\n",
      "Epoch 5/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 25.4497 - dense_4_loss: 13.4468 - dense_5_loss: 12.0029 - dense_4_root_mean_squared_error: 3.6670 - dense_5_root_mean_squared_error: 3.4645\n",
      "Epoch 6/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 25.1141 - dense_4_loss: 13.2355 - dense_5_loss: 11.8787 - dense_4_root_mean_squared_error: 3.6381 - dense_5_root_mean_squared_error: 3.4465\n",
      "Epoch 7/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 25.2152 - dense_4_loss: 13.2707 - dense_5_loss: 11.9445 - dense_4_root_mean_squared_error: 3.6429 - dense_5_root_mean_squared_error: 3.4561\n",
      "Epoch 8/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 25.0225 - dense_4_loss: 13.1420 - dense_5_loss: 11.8805 - dense_4_root_mean_squared_error: 3.6252 - dense_5_root_mean_squared_error: 3.4468\n",
      "Epoch 9/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 25.3696 - dense_4_loss: 13.2676 - dense_5_loss: 12.1020 - dense_4_root_mean_squared_error: 3.6425 - dense_5_root_mean_squared_error: 3.4788\n",
      "Epoch 10/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 24.9741 - dense_4_loss: 13.0747 - dense_5_loss: 11.8994 - dense_4_root_mean_squared_error: 3.6159 - dense_5_root_mean_squared_error: 3.4496\n",
      "Epoch 11/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 24.7045 - dense_4_loss: 12.9310 - dense_5_loss: 11.7734 - dense_4_root_mean_squared_error: 3.5960 - dense_5_root_mean_squared_error: 3.4312\n",
      "Epoch 12/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 24.7656 - dense_4_loss: 12.9472 - dense_5_loss: 11.8184 - dense_4_root_mean_squared_error: 3.5982 - dense_5_root_mean_squared_error: 3.4378\n",
      "Epoch 13/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 24.6832 - dense_4_loss: 12.8706 - dense_5_loss: 11.8126 - dense_4_root_mean_squared_error: 3.5876 - dense_5_root_mean_squared_error: 3.4369\n",
      "Epoch 14/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 24.4246 - dense_4_loss: 12.7344 - dense_5_loss: 11.6902 - dense_4_root_mean_squared_error: 3.5685 - dense_5_root_mean_squared_error: 3.4191\n",
      "Epoch 15/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 24.5838 - dense_4_loss: 12.7770 - dense_5_loss: 11.8068 - dense_4_root_mean_squared_error: 3.5745 - dense_5_root_mean_squared_error: 3.4361\n",
      "Epoch 16/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 24.3982 - dense_4_loss: 12.6653 - dense_5_loss: 11.7329 - dense_4_root_mean_squared_error: 3.5588 - dense_5_root_mean_squared_error: 3.4253\n",
      "Epoch 17/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 24.3304 - dense_4_loss: 12.6269 - dense_5_loss: 11.7035 - dense_4_root_mean_squared_error: 3.5534 - dense_5_root_mean_squared_error: 3.4210\n",
      "Epoch 18/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 24.2144 - dense_4_loss: 12.5359 - dense_5_loss: 11.6785 - dense_4_root_mean_squared_error: 3.5406 - dense_5_root_mean_squared_error: 3.4174\n",
      "Epoch 19/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 24.6555 - dense_4_loss: 12.7252 - dense_5_loss: 11.9303 - dense_4_root_mean_squared_error: 3.5672 - dense_5_root_mean_squared_error: 3.4540\n",
      "Epoch 20/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 24.0874 - dense_4_loss: 12.4614 - dense_5_loss: 11.6260 - dense_4_root_mean_squared_error: 3.5301 - dense_5_root_mean_squared_error: 3.4097\n",
      "Epoch 21/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 24.0431 - dense_4_loss: 12.4120 - dense_5_loss: 11.6311 - dense_4_root_mean_squared_error: 3.5231 - dense_5_root_mean_squared_error: 3.4104\n",
      "Epoch 22/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 24.0578 - dense_4_loss: 12.3665 - dense_5_loss: 11.6913 - dense_4_root_mean_squared_error: 3.5166 - dense_5_root_mean_squared_error: 3.4193\n",
      "Epoch 23/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 24.1106 - dense_4_loss: 12.3701 - dense_5_loss: 11.7405 - dense_4_root_mean_squared_error: 3.5171 - dense_5_root_mean_squared_error: 3.4264\n",
      "Epoch 24/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 23.8642 - dense_4_loss: 12.2491 - dense_5_loss: 11.6151 - dense_4_root_mean_squared_error: 3.4999 - dense_5_root_mean_squared_error: 3.4081\n",
      "Epoch 25/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 23.8390 - dense_4_loss: 12.1971 - dense_5_loss: 11.6419 - dense_4_root_mean_squared_error: 3.4924 - dense_5_root_mean_squared_error: 3.4120\n",
      "Epoch 26/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 23.6244 - dense_4_loss: 12.0913 - dense_5_loss: 11.5332 - dense_4_root_mean_squared_error: 3.4772 - dense_5_root_mean_squared_error: 3.3961\n",
      "Epoch 27/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 23.4687 - dense_4_loss: 12.1719 - dense_5_loss: 11.2967 - dense_4_root_mean_squared_error: 3.4888 - dense_5_root_mean_squared_error: 3.3611\n",
      "Epoch 28/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 23.3283 - dense_4_loss: 12.1813 - dense_5_loss: 11.1471 - dense_4_root_mean_squared_error: 3.4902 - dense_5_root_mean_squared_error: 3.3387\n",
      "Epoch 29/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 23.3336 - dense_4_loss: 12.1964 - dense_5_loss: 11.1372 - dense_4_root_mean_squared_error: 3.4923 - dense_5_root_mean_squared_error: 3.3372\n",
      "Epoch 30/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 23.5233 - dense_4_loss: 12.2344 - dense_5_loss: 11.2888 - dense_4_root_mean_squared_error: 3.4978 - dense_5_root_mean_squared_error: 3.3599\n",
      "Epoch 31/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 23.0267 - dense_4_loss: 11.9955 - dense_5_loss: 11.0312 - dense_4_root_mean_squared_error: 3.4635 - dense_5_root_mean_squared_error: 3.3213\n",
      "Epoch 32/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 23.7036 - dense_4_loss: 12.1713 - dense_5_loss: 11.5324 - dense_4_root_mean_squared_error: 3.4887 - dense_5_root_mean_squared_error: 3.3959\n",
      "Epoch 33/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 23.0891 - dense_4_loss: 11.9843 - dense_5_loss: 11.1048 - dense_4_root_mean_squared_error: 3.4618 - dense_5_root_mean_squared_error: 3.3324\n",
      "Epoch 34/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 22.9274 - dense_4_loss: 11.8960 - dense_5_loss: 11.0315 - dense_4_root_mean_squared_error: 3.4491 - dense_5_root_mean_squared_error: 3.3214\n",
      "Epoch 35/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 22.8938 - dense_4_loss: 11.7972 - dense_5_loss: 11.0966 - dense_4_root_mean_squared_error: 3.4347 - dense_5_root_mean_squared_error: 3.3312\n",
      "Epoch 36/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 22.8676 - dense_4_loss: 11.7861 - dense_5_loss: 11.0815 - dense_4_root_mean_squared_error: 3.4331 - dense_5_root_mean_squared_error: 3.3289\n",
      "Epoch 37/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 23.1429 - dense_4_loss: 11.9629 - dense_5_loss: 11.1801 - dense_4_root_mean_squared_error: 3.4587 - dense_5_root_mean_squared_error: 3.3437\n",
      "Epoch 38/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 23.2062 - dense_4_loss: 11.9421 - dense_5_loss: 11.2642 - dense_4_root_mean_squared_error: 3.4557 - dense_5_root_mean_squared_error: 3.3562\n",
      "Epoch 39/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 23.1370 - dense_4_loss: 11.8889 - dense_5_loss: 11.2482 - dense_4_root_mean_squared_error: 3.4480 - dense_5_root_mean_squared_error: 3.3538\n",
      "Epoch 40/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 22.7290 - dense_4_loss: 11.7032 - dense_5_loss: 11.0258 - dense_4_root_mean_squared_error: 3.4210 - dense_5_root_mean_squared_error: 3.3205\n",
      "Epoch 41/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 22.6590 - dense_4_loss: 11.6463 - dense_5_loss: 11.0127 - dense_4_root_mean_squared_error: 3.4127 - dense_5_root_mean_squared_error: 3.3185\n",
      "Epoch 42/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 22.7577 - dense_4_loss: 11.7350 - dense_5_loss: 11.0226 - dense_4_root_mean_squared_error: 3.4256 - dense_5_root_mean_squared_error: 3.3200\n",
      "Epoch 43/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 22.5109 - dense_4_loss: 11.5338 - dense_5_loss: 10.9771 - dense_4_root_mean_squared_error: 3.3961 - dense_5_root_mean_squared_error: 3.3132\n",
      "Epoch 44/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 22.5656 - dense_4_loss: 11.5747 - dense_5_loss: 10.9909 - dense_4_root_mean_squared_error: 3.4022 - dense_5_root_mean_squared_error: 3.3153\n",
      "Epoch 45/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 22.4777 - dense_4_loss: 11.5227 - dense_5_loss: 10.9550 - dense_4_root_mean_squared_error: 3.3945 - dense_5_root_mean_squared_error: 3.3098\n",
      "Epoch 46/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 22.3697 - dense_4_loss: 11.4522 - dense_5_loss: 10.9174 - dense_4_root_mean_squared_error: 3.3841 - dense_5_root_mean_squared_error: 3.3042\n",
      "Epoch 47/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 22.1786 - dense_4_loss: 11.3365 - dense_5_loss: 10.8421 - dense_4_root_mean_squared_error: 3.3670 - dense_5_root_mean_squared_error: 3.2927\n",
      "Epoch 48/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 22.3036 - dense_4_loss: 11.3877 - dense_5_loss: 10.9159 - dense_4_root_mean_squared_error: 3.3746 - dense_5_root_mean_squared_error: 3.3039\n",
      "Epoch 49/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 22.4405 - dense_4_loss: 11.4088 - dense_5_loss: 11.0318 - dense_4_root_mean_squared_error: 3.3777 - dense_5_root_mean_squared_error: 3.3214\n",
      "Epoch 50/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 22.3188 - dense_4_loss: 11.3331 - dense_5_loss: 10.9857 - dense_4_root_mean_squared_error: 3.3665 - dense_5_root_mean_squared_error: 3.3145\n",
      "Epoch 51/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 22.0665 - dense_4_loss: 11.2194 - dense_5_loss: 10.8471 - dense_4_root_mean_squared_error: 3.3495 - dense_5_root_mean_squared_error: 3.2935\n",
      "Epoch 52/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 22.0880 - dense_4_loss: 11.2064 - dense_5_loss: 10.8815 - dense_4_root_mean_squared_error: 3.3476 - dense_5_root_mean_squared_error: 3.2987\n",
      "Epoch 53/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 22.3566 - dense_4_loss: 11.2806 - dense_5_loss: 11.0760 - dense_4_root_mean_squared_error: 3.3587 - dense_5_root_mean_squared_error: 3.3281\n",
      "Epoch 54/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 22.5439 - dense_4_loss: 11.3947 - dense_5_loss: 11.1492 - dense_4_root_mean_squared_error: 3.3756 - dense_5_root_mean_squared_error: 3.3390\n",
      "Epoch 55/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 22.1609 - dense_4_loss: 11.2058 - dense_5_loss: 10.9552 - dense_4_root_mean_squared_error: 3.3475 - dense_5_root_mean_squared_error: 3.30992s - loss: 26.1118 - dense_4_loss: 12.5643 - dense_5_loss: 13.5474 - dense_4_roo\n",
      "Epoch 56/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 22.3478 - dense_4_loss: 11.2510 - dense_5_loss: 11.0968 - dense_4_root_mean_squared_error: 3.3542 - dense_5_root_mean_squared_error: 3.3312\n",
      "Epoch 57/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 21.7353 - dense_4_loss: 10.9624 - dense_5_loss: 10.7729 - dense_4_root_mean_squared_error: 3.3109 - dense_5_root_mean_squared_error: 3.2822\n",
      "Epoch 58/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 22.0248 - dense_4_loss: 11.0940 - dense_5_loss: 10.9309 - dense_4_root_mean_squared_error: 3.3308 - dense_5_root_mean_squared_error: 3.3062\n",
      "Epoch 59/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 21.6723 - dense_4_loss: 10.9323 - dense_5_loss: 10.7400 - dense_4_root_mean_squared_error: 3.3064 - dense_5_root_mean_squared_error: 3.2772\n",
      "Epoch 60/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 21.6355 - dense_4_loss: 10.8753 - dense_5_loss: 10.7603 - dense_4_root_mean_squared_error: 3.2978 - dense_5_root_mean_squared_error: 3.2803\n",
      "Epoch 61/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 21.5585 - dense_4_loss: 10.8413 - dense_5_loss: 10.7172 - dense_4_root_mean_squared_error: 3.2926 - dense_5_root_mean_squared_error: 3.2737\n",
      "Epoch 62/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 21.4601 - dense_4_loss: 10.7535 - dense_5_loss: 10.7066 - dense_4_root_mean_squared_error: 3.2793 - dense_5_root_mean_squared_error: 3.2721\n",
      "Epoch 63/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 21.5714 - dense_4_loss: 10.8211 - dense_5_loss: 10.7503 - dense_4_root_mean_squared_error: 3.2895 - dense_5_root_mean_squared_error: 3.2788\n",
      "Epoch 64/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 21.8428 - dense_4_loss: 10.9374 - dense_5_loss: 10.9054 - dense_4_root_mean_squared_error: 3.3072 - dense_5_root_mean_squared_error: 3.3023\n",
      "Epoch 65/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 21.2918 - dense_4_loss: 10.6469 - dense_5_loss: 10.6449 - dense_4_root_mean_squared_error: 3.2630 - dense_5_root_mean_squared_error: 3.2627\n",
      "Epoch 66/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 21.4850 - dense_4_loss: 10.7266 - dense_5_loss: 10.7584 - dense_4_root_mean_squared_error: 3.2752 - dense_5_root_mean_squared_error: 3.2800\n",
      "Epoch 67/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 21.3753 - dense_4_loss: 10.6094 - dense_5_loss: 10.7660 - dense_4_root_mean_squared_error: 3.2572 - dense_5_root_mean_squared_error: 3.2812\n",
      "Epoch 68/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 21.2182 - dense_4_loss: 10.5822 - dense_5_loss: 10.6360 - dense_4_root_mean_squared_error: 3.2530 - dense_5_root_mean_squared_error: 3.2613\n",
      "Epoch 69/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 21.3997 - dense_4_loss: 10.6208 - dense_5_loss: 10.7789 - dense_4_root_mean_squared_error: 3.2590 - dense_5_root_mean_squared_error: 3.2831\n",
      "Epoch 70/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 21.4263 - dense_4_loss: 10.6837 - dense_5_loss: 10.7425 - dense_4_root_mean_squared_error: 3.2686 - dense_5_root_mean_squared_error: 3.2776\n",
      "Epoch 71/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 22.1108 - dense_4_loss: 10.9053 - dense_5_loss: 11.2054 - dense_4_root_mean_squared_error: 3.3023 - dense_5_root_mean_squared_error: 3.3475\n",
      "Epoch 72/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 22.0727 - dense_4_loss: 10.9265 - dense_5_loss: 11.1463 - dense_4_root_mean_squared_error: 3.3055 - dense_5_root_mean_squared_error: 3.3386\n",
      "Epoch 73/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 21.0610 - dense_4_loss: 10.4245 - dense_5_loss: 10.6365 - dense_4_root_mean_squared_error: 3.2287 - dense_5_root_mean_squared_error: 3.2614\n",
      "Epoch 74/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 21.2308 - dense_4_loss: 10.5360 - dense_5_loss: 10.6947 - dense_4_root_mean_squared_error: 3.2459 - dense_5_root_mean_squared_error: 3.2703\n",
      "Epoch 75/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 21.2072 - dense_4_loss: 10.5004 - dense_5_loss: 10.7069 - dense_4_root_mean_squared_error: 3.2404 - dense_5_root_mean_squared_error: 3.2721\n",
      "Epoch 76/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 21.1868 - dense_4_loss: 10.4516 - dense_5_loss: 10.7352 - dense_4_root_mean_squared_error: 3.2329 - dense_5_root_mean_squared_error: 3.2765\n",
      "Epoch 77/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 20.8277 - dense_4_loss: 10.2820 - dense_5_loss: 10.5457 - dense_4_root_mean_squared_error: 3.2066 - dense_5_root_mean_squared_error: 3.2474\n",
      "Epoch 78/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 21.0356 - dense_4_loss: 10.3669 - dense_5_loss: 10.6687 - dense_4_root_mean_squared_error: 3.2198 - dense_5_root_mean_squared_error: 3.2663\n",
      "Epoch 79/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 20.8342 - dense_4_loss: 10.3075 - dense_5_loss: 10.5268 - dense_4_root_mean_squared_error: 3.2105 - dense_5_root_mean_squared_error: 3.2445\n",
      "Epoch 80/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 20.7728 - dense_4_loss: 10.2570 - dense_5_loss: 10.5158 - dense_4_root_mean_squared_error: 3.2027 - dense_5_root_mean_squared_error: 3.2428\n",
      "Epoch 81/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 20.9716 - dense_4_loss: 10.3672 - dense_5_loss: 10.6044 - dense_4_root_mean_squared_error: 3.2198 - dense_5_root_mean_squared_error: 3.2564\n",
      "Epoch 82/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 20.9955 - dense_4_loss: 10.3461 - dense_5_loss: 10.6494 - dense_4_root_mean_squared_error: 3.2165 - dense_5_root_mean_squared_error: 3.2633\n",
      "Epoch 83/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 21.2912 - dense_4_loss: 10.3707 - dense_5_loss: 10.9205 - dense_4_root_mean_squared_error: 3.2204 - dense_5_root_mean_squared_error: 3.3046\n",
      "Epoch 84/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 20.5388 - dense_4_loss: 10.1055 - dense_5_loss: 10.4332 - dense_4_root_mean_squared_error: 3.1789 - dense_5_root_mean_squared_error: 3.2301\n",
      "Epoch 85/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 20.5855 - dense_4_loss: 10.0585 - dense_5_loss: 10.5271 - dense_4_root_mean_squared_error: 3.1715 - dense_5_root_mean_squared_error: 3.2445\n",
      "Epoch 86/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 20.7369 - dense_4_loss: 10.1626 - dense_5_loss: 10.5743 - dense_4_root_mean_squared_error: 3.1879 - dense_5_root_mean_squared_error: 3.2518\n",
      "Epoch 87/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 20.8203 - dense_4_loss: 10.2280 - dense_5_loss: 10.5923 - dense_4_root_mean_squared_error: 3.1981 - dense_5_root_mean_squared_error: 3.2546\n",
      "Epoch 88/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 20.4329 - dense_4_loss: 10.0125 - dense_5_loss: 10.4204 - dense_4_root_mean_squared_error: 3.1643 - dense_5_root_mean_squared_error: 3.2281\n",
      "Epoch 89/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 20.4065 - dense_4_loss: 9.9715 - dense_5_loss: 10.4350 - dense_4_root_mean_squared_error: 3.1578 - dense_5_root_mean_squared_error: 3.2303\n",
      "Epoch 90/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 20.2934 - dense_4_loss: 9.9049 - dense_5_loss: 10.3885 - dense_4_root_mean_squared_error: 3.1472 - dense_5_root_mean_squared_error: 3.2231\n",
      "Epoch 91/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 20.3496 - dense_4_loss: 9.9363 - dense_5_loss: 10.4134 - dense_4_root_mean_squared_error: 3.1522 - dense_5_root_mean_squared_error: 3.2270\n",
      "Epoch 92/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 20.1669 - dense_4_loss: 9.8519 - dense_5_loss: 10.3151 - dense_4_root_mean_squared_error: 3.1388 - dense_5_root_mean_squared_error: 3.2117\n",
      "Epoch 93/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 20.2403 - dense_4_loss: 9.8743 - dense_5_loss: 10.3660 - dense_4_root_mean_squared_error: 3.1423 - dense_5_root_mean_squared_error: 3.2196\n",
      "Epoch 94/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 20.4621 - dense_4_loss: 10.0211 - dense_5_loss: 10.4410 - dense_4_root_mean_squared_error: 3.1656 - dense_5_root_mean_squared_error: 3.2312\n",
      "Epoch 95/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 20.2026 - dense_4_loss: 9.8565 - dense_5_loss: 10.3461 - dense_4_root_mean_squared_error: 3.1395 - dense_5_root_mean_squared_error: 3.2165\n",
      "Epoch 96/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 20.2990 - dense_4_loss: 9.8513 - dense_5_loss: 10.4477 - dense_4_root_mean_squared_error: 3.1387 - dense_5_root_mean_squared_error: 3.2323\n",
      "Epoch 97/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 19.9647 - dense_4_loss: 9.7324 - dense_5_loss: 10.2323 - dense_4_root_mean_squared_error: 3.1197 - dense_5_root_mean_squared_error: 3.1988\n",
      "Epoch 98/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 20.3427 - dense_4_loss: 9.8579 - dense_5_loss: 10.4848 - dense_4_root_mean_squared_error: 3.1397 - dense_5_root_mean_squared_error: 3.2380\n",
      "Epoch 99/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 19.9544 - dense_4_loss: 9.6739 - dense_5_loss: 10.2805 - dense_4_root_mean_squared_error: 3.1103 - dense_5_root_mean_squared_error: 3.2063\n",
      "Epoch 100/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 20.7014 - dense_4_loss: 10.0542 - dense_5_loss: 10.6472 - dense_4_root_mean_squared_error: 3.1708 - dense_5_root_mean_squared_error: 3.2630\n",
      "Epoch 101/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 20.4664 - dense_4_loss: 9.9051 - dense_5_loss: 10.5612 - dense_4_root_mean_squared_error: 3.1472 - dense_5_root_mean_squared_error: 3.2498\n",
      "Epoch 102/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 19.7556 - dense_4_loss: 9.5671 - dense_5_loss: 10.1884 - dense_4_root_mean_squared_error: 3.0931 - dense_5_root_mean_squared_error: 3.1919\n",
      "Epoch 103/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 19.8756 - dense_4_loss: 9.6312 - dense_5_loss: 10.2443 - dense_4_root_mean_squared_error: 3.1034 - dense_5_root_mean_squared_error: 3.2007\n",
      "Epoch 104/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 19.7605 - dense_4_loss: 9.5705 - dense_5_loss: 10.1899 - dense_4_root_mean_squared_error: 3.0936 - dense_5_root_mean_squared_error: 3.1922\n",
      "Epoch 105/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 19.8203 - dense_4_loss: 9.5979 - dense_5_loss: 10.2224 - dense_4_root_mean_squared_error: 3.0981 - dense_5_root_mean_squared_error: 3.1973\n",
      "Epoch 106/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 19.8106 - dense_4_loss: 9.5755 - dense_5_loss: 10.2351 - dense_4_root_mean_squared_error: 3.0944 - dense_5_root_mean_squared_error: 3.1992\n",
      "Epoch 107/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 19.7276 - dense_4_loss: 9.5562 - dense_5_loss: 10.1714 - dense_4_root_mean_squared_error: 3.0913 - dense_5_root_mean_squared_error: 3.1893\n",
      "Epoch 108/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 19.4822 - dense_4_loss: 9.4023 - dense_5_loss: 10.0799 - dense_4_root_mean_squared_error: 3.0663 - dense_5_root_mean_squared_error: 3.1749\n",
      "Epoch 109/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 19.7109 - dense_4_loss: 9.4788 - dense_5_loss: 10.2321 - dense_4_root_mean_squared_error: 3.0788 - dense_5_root_mean_squared_error: 3.1988\n",
      "Epoch 110/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 19.6442 - dense_4_loss: 9.4349 - dense_5_loss: 10.2093 - dense_4_root_mean_squared_error: 3.0716 - dense_5_root_mean_squared_error: 3.1952\n",
      "Epoch 111/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 19.5969 - dense_4_loss: 9.4466 - dense_5_loss: 10.1503 - dense_4_root_mean_squared_error: 3.0735 - dense_5_root_mean_squared_error: 3.1860\n",
      "Epoch 112/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 19.2234 - dense_4_loss: 9.2259 - dense_5_loss: 9.9975 - dense_4_root_mean_squared_error: 3.0374 - dense_5_root_mean_squared_error: 3.1619\n",
      "Epoch 113/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 19.3857 - dense_4_loss: 9.3361 - dense_5_loss: 10.0496 - dense_4_root_mean_squared_error: 3.0555 - dense_5_root_mean_squared_error: 3.1701\n",
      "Epoch 114/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 19.7542 - dense_4_loss: 9.3767 - dense_5_loss: 10.3775 - dense_4_root_mean_squared_error: 3.0621 - dense_5_root_mean_squared_error: 3.2214\n",
      "Epoch 115/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 19.3680 - dense_4_loss: 9.3005 - dense_5_loss: 10.0675 - dense_4_root_mean_squared_error: 3.0497 - dense_5_root_mean_squared_error: 3.1729\n",
      "Epoch 116/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 19.4792 - dense_4_loss: 9.2937 - dense_5_loss: 10.1855 - dense_4_root_mean_squared_error: 3.0486 - dense_5_root_mean_squared_error: 3.1915\n",
      "Epoch 117/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 19.3173 - dense_4_loss: 9.2591 - dense_5_loss: 10.0582 - dense_4_root_mean_squared_error: 3.0429 - dense_5_root_mean_squared_error: 3.1715\n",
      "Epoch 118/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 19.1821 - dense_4_loss: 9.1865 - dense_5_loss: 9.9957 - dense_4_root_mean_squared_error: 3.0309 - dense_5_root_mean_squared_error: 3.1616\n",
      "Epoch 119/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 19.1193 - dense_4_loss: 9.1805 - dense_5_loss: 9.9389 - dense_4_root_mean_squared_error: 3.0299 - dense_5_root_mean_squared_error: 3.1526\n",
      "Epoch 120/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 19.1259 - dense_4_loss: 9.1423 - dense_5_loss: 9.9836 - dense_4_root_mean_squared_error: 3.0236 - dense_5_root_mean_squared_error: 3.1597\n",
      "Epoch 121/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 19.3377 - dense_4_loss: 9.2763 - dense_5_loss: 10.0614 - dense_4_root_mean_squared_error: 3.0457 - dense_5_root_mean_squared_error: 3.1720\n",
      "Epoch 122/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 19.0998 - dense_4_loss: 9.1293 - dense_5_loss: 9.9705 - dense_4_root_mean_squared_error: 3.0215 - dense_5_root_mean_squared_error: 3.1576\n",
      "Epoch 123/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 19.3119 - dense_4_loss: 9.1801 - dense_5_loss: 10.1318 - dense_4_root_mean_squared_error: 3.0299 - dense_5_root_mean_squared_error: 3.1830\n",
      "Epoch 124/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 19.2353 - dense_4_loss: 9.1938 - dense_5_loss: 10.0415 - dense_4_root_mean_squared_error: 3.0321 - dense_5_root_mean_squared_error: 3.1688\n",
      "Epoch 125/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 19.0406 - dense_4_loss: 9.0792 - dense_5_loss: 9.9614 - dense_4_root_mean_squared_error: 3.0132 - dense_5_root_mean_squared_error: 3.1562\n",
      "Epoch 126/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 19.0089 - dense_4_loss: 9.0907 - dense_5_loss: 9.9182 - dense_4_root_mean_squared_error: 3.0151 - dense_5_root_mean_squared_error: 3.1493\n",
      "Epoch 127/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 18.8591 - dense_4_loss: 9.0032 - dense_5_loss: 9.8559 - dense_4_root_mean_squared_error: 3.0005 - dense_5_root_mean_squared_error: 3.1394\n",
      "Epoch 128/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 19.1624 - dense_4_loss: 9.0578 - dense_5_loss: 10.1046 - dense_4_root_mean_squared_error: 3.0096 - dense_5_root_mean_squared_error: 3.1788\n",
      "Epoch 129/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 19.0088 - dense_4_loss: 9.0298 - dense_5_loss: 9.9790 - dense_4_root_mean_squared_error: 3.0050 - dense_5_root_mean_squared_error: 3.1590\n",
      "Epoch 130/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 18.9736 - dense_4_loss: 8.9931 - dense_5_loss: 9.9806 - dense_4_root_mean_squared_error: 2.9988 - dense_5_root_mean_squared_error: 3.1592\n",
      "Epoch 131/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 19.1920 - dense_4_loss: 9.0240 - dense_5_loss: 10.1680 - dense_4_root_mean_squared_error: 3.0040 - dense_5_root_mean_squared_error: 3.1887\n",
      "Epoch 132/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 18.8144 - dense_4_loss: 8.9062 - dense_5_loss: 9.9081 - dense_4_root_mean_squared_error: 2.9843 - dense_5_root_mean_squared_error: 3.1477\n",
      "Epoch 133/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 18.5295 - dense_4_loss: 8.7931 - dense_5_loss: 9.7364 - dense_4_root_mean_squared_error: 2.9653 - dense_5_root_mean_squared_error: 3.1203\n",
      "Epoch 134/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 18.7132 - dense_4_loss: 8.8385 - dense_5_loss: 9.8747 - dense_4_root_mean_squared_error: 2.9730 - dense_5_root_mean_squared_error: 3.1424\n",
      "Epoch 135/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 18.6257 - dense_4_loss: 8.8286 - dense_5_loss: 9.7971 - dense_4_root_mean_squared_error: 2.9713 - dense_5_root_mean_squared_error: 3.1300\n",
      "Epoch 136/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 18.5781 - dense_4_loss: 8.7893 - dense_5_loss: 9.7888 - dense_4_root_mean_squared_error: 2.9647 - dense_5_root_mean_squared_error: 3.1287\n",
      "Epoch 137/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 18.4970 - dense_4_loss: 8.8011 - dense_5_loss: 9.6959 - dense_4_root_mean_squared_error: 2.9667 - dense_5_root_mean_squared_error: 3.1138\n",
      "Epoch 138/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 18.4012 - dense_4_loss: 8.7379 - dense_5_loss: 9.6632 - dense_4_root_mean_squared_error: 2.9560 - dense_5_root_mean_squared_error: 3.1086\n",
      "Epoch 139/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 18.6254 - dense_4_loss: 8.8170 - dense_5_loss: 9.8084 - dense_4_root_mean_squared_error: 2.9694 - dense_5_root_mean_squared_error: 3.1318\n",
      "Epoch 140/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 18.3359 - dense_4_loss: 8.6638 - dense_5_loss: 9.6721 - dense_4_root_mean_squared_error: 2.9434 - dense_5_root_mean_squared_error: 3.1100\n",
      "Epoch 141/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 18.7607 - dense_4_loss: 8.8887 - dense_5_loss: 9.8721 - dense_4_root_mean_squared_error: 2.9814 - dense_5_root_mean_squared_error: 3.1420\n",
      "Epoch 142/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 18.3378 - dense_4_loss: 8.6432 - dense_5_loss: 9.6945 - dense_4_root_mean_squared_error: 2.9399 - dense_5_root_mean_squared_error: 3.1136\n",
      "Epoch 143/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 18.3246 - dense_4_loss: 8.6397 - dense_5_loss: 9.6848 - dense_4_root_mean_squared_error: 2.9393 - dense_5_root_mean_squared_error: 3.1120\n",
      "Epoch 144/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 18.3717 - dense_4_loss: 8.6406 - dense_5_loss: 9.7311 - dense_4_root_mean_squared_error: 2.9395 - dense_5_root_mean_squared_error: 3.1195\n",
      "Epoch 145/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 18.3382 - dense_4_loss: 8.6803 - dense_5_loss: 9.6579 - dense_4_root_mean_squared_error: 2.9462 - dense_5_root_mean_squared_error: 3.1077\n",
      "Epoch 146/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 18.2718 - dense_4_loss: 8.6009 - dense_5_loss: 9.6709 - dense_4_root_mean_squared_error: 2.9327 - dense_5_root_mean_squared_error: 3.1098\n",
      "Epoch 147/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 18.3570 - dense_4_loss: 8.6519 - dense_5_loss: 9.7051 - dense_4_root_mean_squared_error: 2.9414 - dense_5_root_mean_squared_error: 3.1153\n",
      "Epoch 148/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 18.5355 - dense_4_loss: 8.7229 - dense_5_loss: 9.8126 - dense_4_root_mean_squared_error: 2.9535 - dense_5_root_mean_squared_error: 3.1325\n",
      "Epoch 149/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 18.1383 - dense_4_loss: 8.5107 - dense_5_loss: 9.6277 - dense_4_root_mean_squared_error: 2.9173 - dense_5_root_mean_squared_error: 3.1028\n",
      "Epoch 150/150\n",
      "1248/1248 [==============================] - 3s 2ms/sample - loss: 18.2068 - dense_4_loss: 8.5991 - dense_5_loss: 9.6078 - dense_4_root_mean_squared_error: 2.9324 - dense_5_root_mean_squared_error: 3.0996\n",
      "Train on 1250 samples\n",
      "Epoch 1/120\n",
      "1250/1250 [==============================] - 3s 2ms/sample - loss: 6.4289 - root_mean_squared_error: 2.5355\n",
      "Epoch 2/120\n",
      "1250/1250 [==============================] - 3s 2ms/sample - loss: 6.6621 - root_mean_squared_error: 2.5811\n",
      "Epoch 3/120\n",
      "1250/1250 [==============================] - 3s 3ms/sample - loss: 6.5236 - root_mean_squared_error: 2.5541\n",
      "Epoch 4/120\n",
      "1250/1250 [==============================] - 3s 3ms/sample - loss: 6.5578 - root_mean_squared_error: 2.5608\n",
      "Epoch 5/120\n",
      "1250/1250 [==============================] - 3s 3ms/sample - loss: 6.4544 - root_mean_squared_error: 2.5406\n",
      "Epoch 6/120\n",
      "1250/1250 [==============================] - 3s 3ms/sample - loss: 6.4836 - root_mean_squared_error: 2.5463\n",
      "Epoch 7/120\n",
      "1250/1250 [==============================] - 3s 3ms/sample - loss: 6.6828 - root_mean_squared_error: 2.5851\n",
      "Epoch 8/120\n",
      "1250/1250 [==============================] - 3s 3ms/sample - loss: 6.7140 - root_mean_squared_error: 2.5911\n",
      "Epoch 9/120\n",
      "1250/1250 [==============================] - 3s 3ms/sample - loss: 6.5840 - root_mean_squared_error: 2.5659\n",
      "Epoch 10/120\n",
      "1250/1250 [==============================] - 3s 3ms/sample - loss: 6.3987 - root_mean_squared_error: 2.5296\n",
      "Epoch 11/120\n",
      "1250/1250 [==============================] - 3s 3ms/sample - loss: 6.3230 - root_mean_squared_error: 2.5146\n",
      "Epoch 12/120\n",
      "1250/1250 [==============================] - 3s 3ms/sample - loss: 6.4588 - root_mean_squared_error: 2.5414\n",
      "Epoch 13/120\n",
      "1250/1250 [==============================] - 3s 3ms/sample - loss: 6.4429 - root_mean_squared_error: 2.5383\n",
      "Epoch 14/120\n",
      "1250/1250 [==============================] - 3s 3ms/sample - loss: 6.4035 - root_mean_squared_error: 2.5305\n",
      "Epoch 15/120\n",
      "1250/1250 [==============================] - 3s 3ms/sample - loss: 6.4853 - root_mean_squared_error: 2.5466\n",
      "Epoch 16/120\n",
      "1250/1250 [==============================] - 3s 3ms/sample - loss: 6.2889 - root_mean_squared_error: 2.5078\n",
      "Epoch 17/120\n",
      "1250/1250 [==============================] - 3s 3ms/sample - loss: 6.3442 - root_mean_squared_error: 2.5188\n",
      "Epoch 18/120\n",
      "1250/1250 [==============================] - 3s 3ms/sample - loss: 6.5008 - root_mean_squared_error: 2.5497\n",
      "Epoch 19/120\n",
      "1250/1250 [==============================] - 3s 3ms/sample - loss: 6.5770 - root_mean_squared_error: 2.5646\n",
      "Epoch 20/120\n",
      "1250/1250 [==============================] - 3s 3ms/sample - loss: 6.4367 - root_mean_squared_error: 2.5371\n",
      "Epoch 21/120\n",
      "1250/1250 [==============================] - 3s 3ms/sample - loss: 6.7486 - root_mean_squared_error: 2.5978\n",
      "Epoch 22/120\n",
      "1250/1250 [==============================] - 3s 3ms/sample - loss: 6.9752 - root_mean_squared_error: 2.6411\n",
      "Epoch 23/120\n",
      "1250/1250 [==============================] - 3s 3ms/sample - loss: 6.6761 - root_mean_squared_error: 2.5838\n",
      "Epoch 24/120\n",
      "1250/1250 [==============================] - 3s 3ms/sample - loss: 6.3545 - root_mean_squared_error: 2.5208\n",
      "Epoch 25/120\n",
      "1250/1250 [==============================] - 3s 3ms/sample - loss: 6.3285 - root_mean_squared_error: 2.5156\n",
      "Epoch 26/120\n",
      "1250/1250 [==============================] - 3s 3ms/sample - loss: 6.2561 - root_mean_squared_error: 2.5012\n",
      "Epoch 27/120\n",
      "1250/1250 [==============================] - 3s 3ms/sample - loss: 6.5899 - root_mean_squared_error: 2.5671\n",
      "Epoch 28/120\n",
      "1250/1250 [==============================] - 3s 3ms/sample - loss: 6.9255 - root_mean_squared_error: 2.6316\n",
      "Epoch 29/120\n",
      "1250/1250 [==============================] - 3s 3ms/sample - loss: 6.4271 - root_mean_squared_error: 2.5352\n",
      "Epoch 30/120\n",
      "1250/1250 [==============================] - 3s 3ms/sample - loss: 6.5008 - root_mean_squared_error: 2.5497\n",
      "Epoch 31/120\n",
      "1250/1250 [==============================] - 3s 3ms/sample - loss: 6.2099 - root_mean_squared_error: 2.4920\n",
      "Epoch 32/120\n",
      "1250/1250 [==============================] - 3s 3ms/sample - loss: 6.2964 - root_mean_squared_error: 2.5093\n",
      "Epoch 33/120\n",
      "1250/1250 [==============================] - 3s 3ms/sample - loss: 6.1448 - root_mean_squared_error: 2.4789\n",
      "Epoch 34/120\n",
      "1250/1250 [==============================] - 3s 3ms/sample - loss: 6.1704 - root_mean_squared_error: 2.4840\n",
      "Epoch 35/120\n",
      "1250/1250 [==============================] - 3s 3ms/sample - loss: 6.2613 - root_mean_squared_error: 2.5023\n",
      "Epoch 36/120\n",
      "1250/1250 [==============================] - 3s 3ms/sample - loss: 7.0257 - root_mean_squared_error: 2.6506\n",
      "Epoch 37/120\n",
      "1250/1250 [==============================] - 3s 3ms/sample - loss: 6.1284 - root_mean_squared_error: 2.4756\n",
      "Epoch 38/120\n",
      " 960/1250 [======================>.......] - ETA: 0s - loss: 5.9067 - root_mean_squared_error: 2.4304"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-70f591e20e50>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;31m# this is your option, you can leave it empty.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mtrader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mre_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoReTrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchoosen_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# choosen_model: 選擇使用 model 1 及 model 2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-62fae5662392>\u001b[0m in \u001b[0;36mre_training\u001b[1;34m(self, doReTrain, choosen_model)\u001b[0m\n\u001b[0;32m    127\u001b[0m                                  \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrain_set_1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m                                  \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m120\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m                                  batch_size=32,)\n\u001b[0m\u001b[0;32m    130\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'./model/model1_{self.StartDatetime_str}.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mchoosen_model\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\K_install\\K_vscode\\K_anaconda\\envs\\TF-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mC:\\K_install\\K_vscode\\K_anaconda\\envs\\TF-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\K_install\\K_vscode\\K_anaconda\\envs\\TF-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\K_install\\K_vscode\\K_anaconda\\envs\\TF-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\K_install\\K_vscode\\K_anaconda\\envs\\TF-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\K_install\\K_vscode\\K_anaconda\\envs\\TF-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    604\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32mC:\\K_install\\K_vscode\\K_anaconda\\envs\\TF-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\K_install\\K_vscode\\K_anaconda\\envs\\TF-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\K_install\\K_vscode\\K_anaconda\\envs\\TF-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\K_install\\K_vscode\\K_anaconda\\envs\\TF-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mC:\\K_install\\K_vscode\\K_anaconda\\envs\\TF-gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# === import package ===\n",
    "\n",
    "# === Functions ===\n",
    "\n",
    "# === 環境參數設定 ===\n",
    "OUTPUT_LOG = True # static global variable, 是否輸出環境參數 log 檔? (儲存於 ./log/model_XXX_XXX.txt)\n",
    "FIX_RANDOM_SEED = True # static global variable, 是否要使 numpy 與 tensorflow 都套用相同的亂數種子 (seed 為隨機產生)\n",
    "\n",
    "# Get current date and time\n",
    "datetime_thisRun = datetime.now() \n",
    "datetime_thisRun_str = datetime_thisRun.strftime(\"%Y_%m_%d__%H_%M_%S\")\n",
    "\n",
    "# numpy 與 tensorflow 設定相同的變數種子\n",
    "if FIX_RANDOM_SEED:\n",
    "    # 使用 random BitGenerator 隨機生出一個介於 0 ~ 2**32-1 的整數\n",
    "    rng = np.random.default_rng()\n",
    "    myseed = rng.integers(0b11111111111111111111111111111111, size=1) # numpy seed 為 unsign int32 \n",
    "    np.random.seed(seed = myseed) # follow MT19937 by default, seed range is 0 ~ 2**32 - 1\n",
    "    tf.random.set_seed(myseed)\n",
    "\n",
    "# 儲存實驗參數\n",
    "if OUTPUT_LOG:\n",
    "    # Loggering dirName\n",
    "    log_dirName = f'log/{datetime_thisRun_str}'\n",
    "    # Make dir of Loggering\n",
    "    safely_mkdir(log_dirName)\n",
    "    # 紀錄 datetime\n",
    "    printlog(f\"Date: {datetime_thisRun}\", f'{log_dirName}/log.txt')\n",
    "    # 紀錄 random seed\n",
    "    if OUTPUT_LOG: printlog(f\"Random seed: {myseed}\", f'{log_dirName}/log.txt')\n",
    "\n",
    "# === 以下模擬 if __name__ == \"__main__\": 的部分 ===\n",
    "# The following part is an example.\n",
    "# You can modify it at will.\n",
    "\n",
    "# 若路徑已存在 `output.csv`，刪除它，百分百保證本程式 `沒有使用到舊的 output.csv`\n",
    "# If `output.csv` already exists in the path, delete it and make sure that `the program does not use the old output.csv`\n",
    "try:\n",
    "    os.remove('./output.csv')\n",
    "except OSError as e:\n",
    "    print(f\"This program does not use the old output.csv!!!\")\n",
    "\n",
    "training_data = load_data(\"training_data.csv\")\n",
    "trader = Trader(datetime_thisRun_str) # 代入目前的時間 (datetime_thisRun_str)，純粹用於指定 model 的儲存路徑，不會用於訓練或預測\n",
    "trader.train(training_data, usePreTrain=True, choosen_model=[True, True]) # choosen_model: 選擇使用 model 1 及 model 2\n",
    "\n",
    "testing_data = load_data(\"testing_data.csv\")\n",
    "with open(\"output.csv\", \"w\") as output_file:\n",
    "    for index, row in testing_data.iterrows():\n",
    "        # We will perform your action as the open price in the next day.\n",
    "        action = trader.predict_action(row)\n",
    "        output_file.write(action)\n",
    "\n",
    "        # this is your option, you can leave it empty.\n",
    "        trader.re_training(doReTrain=True, choosen_model=[True, True]) # choosen_model: 選擇使用 model 1 及 model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135e596f",
   "metadata": {},
   "source": [
    "## Main\n",
    "Notice: \n",
    "* 每讀完一天的資料，就必須輸出隔天是否要購買股票，輸出後才能再讀下一天的資料，以此類推。\n",
    "    * E.g.:\n",
    "        * D = 新的一天股票資料產生\n",
    "        * A = 模型執行的動作 (-1, 0, 1)\n",
    "        * 合法的順序： DADADADADA\n",
    "* 違反下列規則 0 分計算:\n",
    "    * 使用非法的測試資料讀取方式，E.g.: DDDDDADDA\n",
    "    * 一次讀完全部股票資料才輸出是否要購買\n",
    "    * 修改輸出結果\n",
    "    * 輸出結果數量不正確\n",
    "    * Your code is the same as other classmate’s.\n",
    "    * Your Github repo does not contain your code.\n",
    "    * You do not submit the homework before the deadline.\n",
    "    * **The training dataset you used is not provided by TA. (言下之意，不能更改/新增 training data)** \n",
    "    * Your code can get data through the Internet during training or testing. (不能使用爬蟲) \n",
    "    * Your code terminates during testing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20119675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mute\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Functions\n",
    "def load_data(load_file_dir:str):\n",
    "    return pd.read_csv(load_file_dir, header=None)\n",
    "\n",
    "# You can write code above the if-main block.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # You should not modify this part.\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--training\", default=\"training_data.csv\", help=\"input training data file name\")\n",
    "    parser.add_argument(\"--testing\", default=\"testing_data.csv\", help=\"input testing data file name\")\n",
    "    parser.add_argument(\"--output\", default=\"output.csv\", help=\"output file name\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # The following part is an example.\n",
    "    # You can modify it at will.\n",
    "    training_data = load_data(args.training)\n",
    "    trader = Trader()\n",
    "    trader.train(training_data)\n",
    "\n",
    "    testing_data = load_data(args.testing)\n",
    "    with open(args.output, \"w\") as output_file:\n",
    "        for index, row in testing_data.iterrows():\n",
    "            # We will perform your action as the open price in the next day.\n",
    "            action = trader.predict_action(row)\n",
    "            output_file.write(action)\n",
    "\n",
    "            # this is your option, you can leave it empty.\n",
    "            trader.re_training()\n",
    "\"\"\"\n",
    "print(\"mute\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f156d77b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d394e5382fbb560cb6f97685d62a08bd8c37a9465edea3419395818d122d3067"
  },
  "kernelspec": {
   "display_name": "DSAI-HW2-py364-JPKernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
